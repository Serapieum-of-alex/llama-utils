
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Llama-utils is a Python package for data manipulation and analysis in the Llama language model">
      
      
      
        <link rel="canonical" href="https://github.com/Serapieum-of-alex/llama-utils/main/tools/ollama.html">
      
      
        <link rel="prev" href="../api/utils.html">
      
      
        <link rel="next" href="llama-cpp.html">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.5">
    
    
      
        <title>Running a Local LLM using Ollama - Llama-utils</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-a-local-llm-using-ollama" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="Llama-utils" class="md-header__button md-logo" aria-label="Llama-utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Llama-utils
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running a Local LLM using Ollama
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="lime"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../dev/installation.html" class="md-tabs__link">
          
  
    
  
  Installation

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../design-architecture-diagrams.html" class="md-tabs__link">
          
  
    
  
  Design

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../api/index_manager.html" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="ollama.html" class="md-tabs__link">
          
  
    
  
  LLM Tools

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="notebooks/llama-cpp.html" class="md-tabs__link">
          
  
    
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../change-log.html" class="md-tabs__link">
        
  
    
  
  Change logs

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="Llama-utils" class="md-nav__button md-logo" aria-label="Llama-utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Llama-utils
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Installation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dev/installation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Design
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Design
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../design-architecture-diagrams.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Design and Architecture Diagrams
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/index_manager.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index Manager
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/storage.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Storage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/pdf_reader.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pdf_reader
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    LLM Tools
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            LLM Tools
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Running a Local LLM using Ollama
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="ollama.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Running a Local LLM using Ollama
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-installation-of-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      1. Installation of Ollama
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Installation of Ollama">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#windows" class="md-nav__link">
    <span class="md-ellipsis">
      Windows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#macos" class="md-nav__link">
    <span class="md-ellipsis">
      macOS
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linux" class="md-nav__link">
    <span class="md-ellipsis">
      Linux
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-downloading-and-managing-models" class="md-nav__link">
    <span class="md-ellipsis">
      2. Downloading and Managing Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Downloading and Managing Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-available-models" class="md-nav__link">
    <span class="md-ellipsis">
      List Available Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#download-a-model" class="md-nav__link">
    <span class="md-ellipsis">
      Download a Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove-a-model" class="md-nav__link">
    <span class="md-ellipsis">
      Remove a Model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-running-ollama-locally" class="md-nav__link">
    <span class="md-ellipsis">
      3. Running Ollama Locally
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Running Ollama Locally">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#start-the-ollama-server" class="md-nav__link">
    <span class="md-ellipsis">
      Start the Ollama Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-gpu-acceleration" class="md-nav__link">
    <span class="md-ellipsis">
      Configure GPU Acceleration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-interacting-with-ollama-via-cli" class="md-nav__link">
    <span class="md-ellipsis">
      4. Interacting with Ollama via CLI
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Interacting with Ollama via CLI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-using-ollama-as-an-api" class="md-nav__link">
    <span class="md-ellipsis">
      5. Using Ollama as an API
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Using Ollama as an API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#starting-the-api" class="md-nav__link">
    <span class="md-ellipsis">
      Starting the API
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1making-a-request-via-curl" class="md-nav__link">
    <span class="md-ellipsis">
      1.Making a Request via curl
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.Making a Request via curl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#options" class="md-nav__link">
    <span class="md-ellipsis">
      Options
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#return_error_traceback-whether-to-return-the-error-traceback-in-the-response-default-false" class="md-nav__link">
    <span class="md-ellipsis">
      return_error_traceback: Whether to return the error traceback in the response (default: false).
    </span>
  </a>
  
    <nav class="md-nav" aria-label="return_error_traceback: Whether to return the error traceback in the response (default: false).">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2using-python-requests" class="md-nav__link">
    <span class="md-ellipsis">
      2.Using Python (requests)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-integrating-ollama-with-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      6. Integrating Ollama with llama-index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Integrating Ollama with llama-index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Install Dependencies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuring-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      Configuring llama-index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#querying-ollama-via-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      Querying Ollama via llama-index
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      7. Performance Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Performance Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1gpu-acceleration" class="md-nav__link">
    <span class="md-ellipsis">
      1.GPU Acceleration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2change-port-number" class="md-nav__link">
    <span class="md-ellipsis">
      2.Change port number
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2change-cache-directory" class="md-nav__link">
    <span class="md-ellipsis">
      2.Change cache directory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.Change cache directory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#change-default-ollama-directory-symlink" class="md-nav__link">
    <span class="md-ellipsis">
      Change default .ollama directory (Symlink)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-common-issues-troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      8. Common Issues &amp; Troubleshooting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9errors" class="md-nav__link">
    <span class="md-ellipsis">
      9.Errors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.Errors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1address-not-permitted" class="md-nav__link">
    <span class="md-ellipsis">
      1.address not permitted
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="llama-cpp.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Llama.cpp for Local LLM Inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="notebooks/llama-cpp.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Llama.cpp for Chat Completion
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../change-log.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change logs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-installation-of-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      1. Installation of Ollama
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Installation of Ollama">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#windows" class="md-nav__link">
    <span class="md-ellipsis">
      Windows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#macos" class="md-nav__link">
    <span class="md-ellipsis">
      macOS
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linux" class="md-nav__link">
    <span class="md-ellipsis">
      Linux
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-downloading-and-managing-models" class="md-nav__link">
    <span class="md-ellipsis">
      2. Downloading and Managing Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Downloading and Managing Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-available-models" class="md-nav__link">
    <span class="md-ellipsis">
      List Available Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#download-a-model" class="md-nav__link">
    <span class="md-ellipsis">
      Download a Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove-a-model" class="md-nav__link">
    <span class="md-ellipsis">
      Remove a Model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-running-ollama-locally" class="md-nav__link">
    <span class="md-ellipsis">
      3. Running Ollama Locally
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Running Ollama Locally">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#start-the-ollama-server" class="md-nav__link">
    <span class="md-ellipsis">
      Start the Ollama Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-gpu-acceleration" class="md-nav__link">
    <span class="md-ellipsis">
      Configure GPU Acceleration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-interacting-with-ollama-via-cli" class="md-nav__link">
    <span class="md-ellipsis">
      4. Interacting with Ollama via CLI
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Interacting with Ollama via CLI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-using-ollama-as-an-api" class="md-nav__link">
    <span class="md-ellipsis">
      5. Using Ollama as an API
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Using Ollama as an API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#starting-the-api" class="md-nav__link">
    <span class="md-ellipsis">
      Starting the API
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1making-a-request-via-curl" class="md-nav__link">
    <span class="md-ellipsis">
      1.Making a Request via curl
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.Making a Request via curl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#options" class="md-nav__link">
    <span class="md-ellipsis">
      Options
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#return_error_traceback-whether-to-return-the-error-traceback-in-the-response-default-false" class="md-nav__link">
    <span class="md-ellipsis">
      return_error_traceback: Whether to return the error traceback in the response (default: false).
    </span>
  </a>
  
    <nav class="md-nav" aria-label="return_error_traceback: Whether to return the error traceback in the response (default: false).">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2using-python-requests" class="md-nav__link">
    <span class="md-ellipsis">
      2.Using Python (requests)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-integrating-ollama-with-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      6. Integrating Ollama with llama-index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Integrating Ollama with llama-index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Install Dependencies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuring-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      Configuring llama-index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#querying-ollama-via-llama-index" class="md-nav__link">
    <span class="md-ellipsis">
      Querying Ollama via llama-index
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      7. Performance Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Performance Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1gpu-acceleration" class="md-nav__link">
    <span class="md-ellipsis">
      1.GPU Acceleration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2change-port-number" class="md-nav__link">
    <span class="md-ellipsis">
      2.Change port number
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2change-cache-directory" class="md-nav__link">
    <span class="md-ellipsis">
      2.Change cache directory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.Change cache directory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#change-default-ollama-directory-symlink" class="md-nav__link">
    <span class="md-ellipsis">
      Change default .ollama directory (Symlink)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-common-issues-troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      8. Common Issues &amp; Troubleshooting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9errors" class="md-nav__link">
    <span class="md-ellipsis">
      9.Errors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.Errors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1address-not-permitted" class="md-nav__link">
    <span class="md-ellipsis">
      1.address not permitted
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="running-a-local-llm-using-ollama">Running a Local LLM using Ollama</h1>
<ul>
<li>Ollama is a tool for running large language models (LLMs) locally on your machine. It allows you to download, run,
  and interact with various LLMs without relying on cloud-based APIs.</li>
<li>Ollama is designed for privacy, performance, and ease of use, making it ideal for offline inference.</li>
</ul>
<p>Key Features of Ollama
- Local Execution – Runs directly on your computer without requiring an internet connection.
- Supports Multiple Models – Includes Mistral, LLaMA 2, Gemma, and more.
- GPU Acceleration – Uses CUDA (NVIDIA) or Metal (Mac) for faster performance.
- API Interface – Exposes a simple HTTP API for easy integration with applications.
- CLI-Based Interaction – Allows running inference from the command line.</p>
<h2 id="1-installation-of-ollama">1. Installation of Ollama</h2>
<h3 id="windows"><strong>Windows</strong></h3>
<ol>
<li><strong>Download the Installer</strong>
   Visit <a href="https://ollama.com">Ollama's website</a> and download the Windows installer.</li>
<li>
<p><strong>Run the Installer</strong></p>
</li>
<li>
<p>Double-click the downloaded <code>.exe</code> file.</p>
</li>
<li>Follow the installation prompts.</li>
<li>Restart your terminal after installation.</li>
<li><strong>Verify Installation</strong>
   Open <strong>PowerShell</strong> or <strong>Command Prompt</strong> and run:</li>
</ol>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>--version
</code></pre></div>
<p>This should return the installed version.</p>
<p align="center">
  <img src="images/ollama/ollama-version.png" alt="Ollama List">
</p>

<h3 id="macos"><strong>macOS</strong></h3>
<ol>
<li><strong>Install via Homebrew</strong></li>
</ol>
<p><div class="highlight"><pre><span></span><code>brew<span class="w"> </span>install<span class="w"> </span>ollama
</code></pre></div>
2. <strong>Verify Installation</strong></p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>--version
</code></pre></div>
<h3 id="linux"><strong>Linux</strong></h3>
<ol>
<li><strong>Install via Curl</strong></li>
</ol>
<p><div class="highlight"><pre><span></span><code>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://ollama.com/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
</code></pre></div>
2. <strong>Verify Installation</strong></p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>--version
</code></pre></div>
<hr />
<h2 id="2-downloading-and-managing-models">2. Downloading and Managing Models</h2>
<h3 id="list-available-models"><strong>List Available Models</strong></h3>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>list
</code></pre></div>
<p align="center">
  <img src="images/ollama/ollama-list.png" alt="Ollama List">
</p>

<h3 id="download-a-model"><strong>Download a Model</strong></h3>
<p>First, you can check the available models at https://ollama.com/library and run the following command to download a model:</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>pull<span class="w"> </span>mistral
ollama<span class="w"> </span>pull<span class="w"> </span>gemma
ollama<span class="w"> </span>pull<span class="w"> </span>llama2
</code></pre></div>
<p align="center">
  <img src="images/ollama/ollama-pull-gemma.png" alt="Ollama Pull">
</p>

<ul>
<li>When a model is downloaded using Ollama, it is stored in the <code>~/.ollama/models/</code> directory. The model files are
located in a subdirectory called <code>blobs</code>. The model's manifest is available in the <code>~/.ollama/models/manifests/registry.ollama.ai/library/&lt;model&gt;/latest</code> file.</li>
</ul>
<p align="center">
  <img src="images/ollama/ollama-models-blobs.png" alt="Ollama download location" width="600">
</p>

<h3 id="remove-a-model"><strong>Remove a Model</strong></h3>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>rm<span class="w"> </span>llama3
</code></pre></div>
<p align="center">
  <img src="images/ollama/ollama-rm.png" alt="Ollama remove">
</p>

<hr />
<h2 id="3-running-ollama-locally">3. Running Ollama Locally</h2>
<h3 id="start-the-ollama-server"><strong>Start the Ollama Server</strong></h3>
<p>Run:</p>
<p><div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>serve
</code></pre></div>
you will get something like</p>
<div class="highlight"><pre><span></span><code>Windows<span class="w"> </span>PowerShell
Copyright<span class="w"> </span><span class="o">(</span>C<span class="o">)</span><span class="w"> </span>Microsoft<span class="w"> </span>Corporation.<span class="w"> </span>All<span class="w"> </span>rights<span class="w"> </span>reserved.

Install<span class="w"> </span>the<span class="w"> </span>latest<span class="w"> </span>PowerShell<span class="w"> </span><span class="k">for</span><span class="w"> </span>new<span class="w"> </span>features<span class="w"> </span>and<span class="w"> </span>improvements!<span class="w"> </span>https://aka.ms/PSWindows

Loading<span class="w"> </span>personal<span class="w"> </span>and<span class="w"> </span>system<span class="w"> </span>profiles<span class="w"> </span>took<span class="w"> </span>2774ms.
<span class="o">(</span>llama-utils-l7lRCPhg-py3.12<span class="o">)(</span>base<span class="o">)</span><span class="w"> </span>C:<span class="se">\g</span>drive<span class="se">\a</span>lgorithms<span class="se">\A</span>I<span class="se">\l</span>lms<span class="se">\l</span>lama-utils<span class="w"> </span>git:<span class="o">[</span>dev/tools-documentations<span class="o">]</span>
ollama<span class="w"> </span>serve
<span class="m">2025</span>/02/01<span class="w"> </span><span class="m">20</span>:05:46<span class="w"> </span>routes.go:1187:<span class="w"> </span>INFO<span class="w"> </span>server<span class="w"> </span>config<span class="w"> </span><span class="nv">env</span><span class="o">=</span><span class="s2">&quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\eng_m\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.724+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:432<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total blobs: 31&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.726+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:439<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total unused blobs removed: 0&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.728+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1238<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Listening on 127.0.0.1:11434 (version 0.5.7)&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.729+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1267<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Dynamic LLM libraries&quot;</span><span class="w"> </span><span class="nv">runners</span><span class="o">=</span><span class="s2">&quot;[cuda_v12_avx rocm_avx cpu cpu_avx cpu_avx2 cuda_v11_avx]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.729+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu.go:226<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;looking for compatible GPUs&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.729+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:167<span class="w"> </span><span class="nv">msg</span><span class="o">=</span>packages<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">1</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.729+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:214<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="nv">package</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">cores</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="nv">efficiency</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">threads</span><span class="o">=</span><span class="m">16</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.956+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu.go:334<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;detected OS VRAM overhead&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-04f76f9a-be0a-544b-9a6f-8607b8d0a9ab<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.6<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.6<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA GeForce RTX 3060 Ti&quot;</span><span class="w"> </span><span class="nv">overhead</span><span class="o">=</span><span class="s2">&quot;283.2 MiB&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T20:05:46.959+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-04f76f9a-be0a-544b-9a6f-8607b8d0a9ab<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.6<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.6<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA GeForce RTX 3060 Ti&quot;</span>
</code></pre></div>
<p align="center">
  <img src="images/ollama/ollama-serve.png" alt="Ollama Serve">
</p>

<p>Check if the server is running:</p>
<div class="highlight"><pre><span></span><code>ps<span class="w"> </span>aux<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>ollama
</code></pre></div>
<p>for Windows use:</p>
<div class="highlight"><pre><span></span><code>tasklist<span class="w"> </span><span class="p">|</span><span class="w"> </span>findstr<span class="w"> </span>/i<span class="w"> </span><span class="s2">&quot;ollama&quot;</span>
</code></pre></div>
<p>you will get something similar to the following
<div class="highlight"><pre><span></span><code>tasklist<span class="w"> </span><span class="p">|</span><span class="w"> </span>findstr<span class="w"> </span>/i<span class="w"> </span><span class="s2">&quot;ollama&quot;</span>
ollama.exe<span class="w">                   </span><span class="m">18460</span><span class="w"> </span>Console<span class="w">                    </span><span class="m">1</span><span class="w">     </span><span class="m">48</span>,644<span class="w"> </span>K
</code></pre></div></p>
<h3 id="configure-gpu-acceleration"><strong>Configure GPU Acceleration</strong></h3>
<p>If your system supports CUDA, Ollama will use the GPU automatically. You can check GPU usage via:</p>
<div class="highlight"><pre><span></span><code>nvidia-smi
</code></pre></div>
<p>To force CPU usage:</p>
<p><div class="highlight"><pre><span></span><code><span class="nv">OLLAMA_USE_CPU</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>ollama<span class="w"> </span>run<span class="w"> </span>mistral
</code></pre></div>
for Windows use:</p>
<div class="highlight"><pre><span></span><code><span class="nb">set</span><span class="w"> </span><span class="nv">OLLAMA_USE_CPU</span><span class="o">=</span><span class="m">1</span>
ollama<span class="w"> </span>run<span class="w"> </span>mistral<span class="w">  </span>
</code></pre></div>
<h3 id="troubleshooting"><strong>Troubleshooting</strong></h3>
<ul>
<li>If <code>ollama</code> commands are not found, restart your shell or check your <code>$PATH</code>:
<div class="highlight"><pre><span></span><code><span class="nb">echo</span><span class="w"> </span><span class="nv">$PATH</span>
</code></pre></div></li>
<li>For permission issues, try running:
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>ollama<span class="w"> </span>serve
</code></pre></div></li>
</ul>
<hr />
<h2 id="4-interacting-with-ollama-via-cli">4. Interacting with Ollama via CLI</h2>
<h3 id="basic-inference"><strong>Basic Inference</strong></h3>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>run<span class="w"> </span>mistral
</code></pre></div>
<p align="center">
  <img src="images/ollama/ollama-run-terminal.png" alt="Ollama Run">
</p>

<p>to exit type
<div class="highlight"><pre><span></span><code>/bye
</code></pre></div></p>
<p>you can also run the following command to get the response from the model directly</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>run<span class="w"> </span>mistral<span class="w"> </span><span class="s2">&quot;What is the capital of France?&quot;</span>
</code></pre></div>
<p>The above command will return the following response</p>
<div class="highlight"><pre><span></span><code>Paris is the capital of France.
</code></pre></div>
<hr />
<h2 id="5-using-ollama-as-an-api">5. Using Ollama as an API</h2>
<h3 id="starting-the-api"><strong>Starting the API</strong></h3>
<p>Run:</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>serve
</code></pre></div>
<h3 id="1making-a-request-via-curl">1.<strong>Making a Request via <code>curl</code></strong></h3>
<p><div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;model&quot;: &quot;mistral&quot;,</span>
<span class="s1">  &quot;prompt&quot;: &quot;Tell me a joke&quot;</span>
<span class="s1">}&#39;</span>
</code></pre></div>
for command terminal in Windows use:
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="s2">&quot;http://127.0.0.1:11434/api/generate&quot;</span><span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;{\&quot;model\&quot;: \&quot;mistral\&quot;, \&quot;prompt\&quot;: \&quot;Tell me a short fact about AI\&quot;, \&quot;options\&quot;: {\&quot;max_tokens\&quot;: 50}}&quot;</span>
</code></pre></div></p>
<p>The response will be similar to:</p>
<div class="highlight"><pre><span></span><code>C:<span class="se">\U</span>sers<span class="se">\m</span>y-user-name&gt;curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="s2">&quot;http://127.0.0.1:11434/api/generate&quot;</span><span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span>
<span class="s2">{\&quot;model\&quot;: \&quot;mistral\&quot;, \&quot;prompt\&quot;: \&quot;Tell me a short fact about AI\&quot;, \&quot;options\&quot;: {\&quot;max_tokens\&quot;: 50}}&quot;</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:25.9015572Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; Art&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:25.9428293Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;ificial&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:25.9610394Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; Intelligence&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:25.9747797Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; (&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:25.9880321Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;AI&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0026846Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;)&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.017711Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; has&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0311171Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; the&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0447043Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; capability&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0593169Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; to&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0724141Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; learn&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.0859434Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; from&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1004478Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; its&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1138166Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; experiences&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1279841Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;,&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1404762Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; adjust&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1544505Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; its&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1684524Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; behavior&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1826811Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;,&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.1958623Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; and&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2081325Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; improve&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2259143Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; its&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2384732Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; performance&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.25283Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; based&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2662188Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; on&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2807515Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; that&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.2942798Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; learning&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3073516Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;.&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3217636Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; This&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3351073Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; is&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.349646Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; known&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3629003Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; as&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3757784Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; machine&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.3892467Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; learning&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4030308Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;,&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4183595Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; one&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4317698Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; of&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4465969Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; the&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4595035Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; sub&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.473006Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;fields&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.4876476Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; of&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.5008632Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot; AI&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.5142572Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;.&quot;</span>,<span class="s2">&quot;done&quot;</span>:false<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="s2">&quot;mistral&quot;</span>,<span class="s2">&quot;created_at&quot;</span>:<span class="s2">&quot;2025-02-01T21:01:26.5306917Z&quot;</span>,<span class="s2">&quot;response&quot;</span>:<span class="s2">&quot;&quot;</span>,<span class="s2">&quot;done&quot;</span>:true,<span class="s2">&quot;done_reason&quot;</span>:<span class="s2">&quot;stop&quot;</span>,<span class="s2">&quot;context&quot;</span>:<span class="o">[</span><span class="m">3</span>,29473,16027,1296,1032,3253,2407,1452,16875,4,1027,4719,15541,23859,1093,12509,29499,1427,1040,22136,1066,3590,1245,1639,9789,29493,8160,1639,6942,29493,1072,5684,1639,5165,3586,1124,1137,5936,29491,1619,1117,3419,1158,6367,5936,29493,1392,1070,1040,1851,8202,1070,16875,29491<span class="o">]</span>,<span class="s2">&quot;total_duration&quot;</span>:4969300700,<span class="s2">&quot;load_duration&quot;</span>:3858876600,<span class="s2">&quot;prompt_eval_count&quot;</span>:12,<span class="s2">&quot;prompt_eval_duration&quot;</span>:470000000,<span class="s2">&quot;eval_count&quot;</span>:44,<span class="s2">&quot;eval_duration&quot;</span>:630000000<span class="o">}</span>
</code></pre></div>
<h4 id="options"><strong>Options</strong></h4>
<ul>
<li><code>model</code>: The LLM model to use (e.g., <code>mistral</code>, <code>gemma</code>, <code>llama2</code>).</li>
<li><code>prompt</code>: The input text or query for the model.</li>
<li><code>options</code>: Additional parameters like <code>max_tokens</code>, <code>temperature</code>, etc.</li>
<li><code>max_tokens</code>: The maximum number of tokens to generate in the response.</li>
<li><code>stream</code>: Whether to stream the response (default: <code>false</code>). If <code>true</code>, the response will be streamed in chunks.</li>
<li><code>temperature</code>: Controls the randomness of the output (default: <code>0.7</code>).</li>
<li><code>top_p</code>: Filters the tokens to consider based on their cumulative probability (default: <code>0.9</code>).</li>
<li><code>top_k</code>: Filters the tokens to consider based on their likelihood (default: <code>50</code>).</li>
<li><code>stop</code>: A list of tokens at which the model should stop generating text.</li>
<li><code>n</code>: The number of completions to generate (default: <code>1</code>).</li>
<li><code>return_full</code>: Whether to return the full response context (default: <code>false</code>).</li>
<li><code>return_prompt</code>: Whether to return the prompt in the response (default: <code>false</code>).</li>
<li><code>return_options</code>: Whether to return the options in the response (default: <code>false</code>).</li>
<li><code>return_context</code>: Whether to return the context in the response (default: <code>false</code>).</li>
<li><code>return_duration</code>: Whether to return the duration in the response (default: <code>false</code>).</li>
<li><code>return_eval_count</code>: Whether to return the evaluation count in the response (default: <code>false</code>).</li>
<li><code>return_eval_duration</code>: Whether to return the evaluation duration in the response (default: <code>false</code>).</li>
<li><code>return_total_duration</code>: Whether to return the total duration in the response (default: <code>false</code>).</li>
<li><code>return_load_duration</code>: Whether to return the load duration in the response (default: <code>false</code>).</li>
<li><code>return_done</code>: Whether to return the done status in the response (default: <code>false</code>).</li>
<li><code>return_done_reason</code>: Whether to return the done reason in the response (default: <code>false</code>).</li>
<li><code>return_error</code>: Whether to return the error in the response (default: <code>false</code>).</li>
<li><code>return_error_message</code>: Whether to return the error message in the response (default: <code>false</code>).</li>
<li>
<h2 id="return_error_traceback-whether-to-return-the-error-traceback-in-the-response-default-false"><code>return_error_traceback</code>: Whether to return the error traceback in the response (default: <code>false</code>).</h2>
</li>
</ul>
<h3 id="2using-python-requests">2.<strong>Using Python (<code>requests</code>)</strong></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:11434/api/generate&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;mistral&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</code></pre></div>
<hr />
<h2 id="6-integrating-ollama-with-llama-index">6. Integrating Ollama with <code>llama-index</code></h2>
<h3 id="install-dependencies"><strong>Install Dependencies</strong></h3>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>llama-index-llms-ollama
</code></pre></div>
<h3 id="configuring-llama-index"><strong>Configuring <code>llama-index</code></strong></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llama_index.llms.ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ollama</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llama_index.core</span><span class="w"> </span><span class="kn">import</span> <span class="n">Settings</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
</code></pre></div>
<h3 id="querying-ollama-via-llama-index"><strong>Querying Ollama via <code>llama-index</code></strong></h3>
<div class="highlight"><pre><span></span><code><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s2">&quot;What is the capital of the Netherlands?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>The capital city of the Netherlands is Amsterdam. However, it&#39;s important to note that The Hague (Den Haag) serves as the seat of government, hosting several key institutions such as the Dutch Parliament and the Supreme Court. Amsterdam, on the other hand, is known for its vibrant culture and economic significance.
</code></pre></div>
<hr />
<h2 id="7-performance-optimization">7. Performance Optimization</h2>
<h3 id="1gpu-acceleration">1.<strong>GPU Acceleration</strong></h3>
<ul>
<li>Ensure you have CUDA installed:
<div class="highlight"><pre><span></span><code>nvidia-smi
</code></pre></div></li>
<li>Run Ollama with GPU support:
<div class="highlight"><pre><span></span><code><span class="nv">OLLAMA_USE_CUDA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>ollama<span class="w"> </span>serve
</code></pre></div>
for Windows use:
<div class="highlight"><pre><span></span><code><span class="nb">set</span><span class="w"> </span><span class="nv">OLLAMA_USE_CUDA</span><span class="o">=</span><span class="m">1</span>
ollama<span class="w"> </span>serve
</code></pre></div></li>
</ul>
<h3 id="2change-port-number">2.<strong>Change port number</strong></h3>
<ul>
<li>To change the port number, use the <code>OLLAMA_HOST</code> environment variable to specify the new port:
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OLLAMA_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1:8081
ollama<span class="w"> </span>serve
</code></pre></div>
for Windows use:
<div class="highlight"><pre><span></span><code><span class="nb">set</span><span class="w"> </span><span class="nv">OLLAMA_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1:8081
ollama<span class="w"> </span>serve
</code></pre></div>
the previous command will change the port to <code>8081</code> from the default port <code>11434</code>.
the output will be similar to the following
<div class="highlight"><pre><span></span><code>C:<span class="se">\U</span>sers<span class="se">\e</span>ng_m&gt;set<span class="w"> </span><span class="nv">OLLAMA_HOST</span><span class="o">=</span>http://127.0.0.1:8081

C:<span class="se">\U</span>sers<span class="se">\e</span>ng_m&gt;ollama<span class="w"> </span>serve
<span class="m">2025</span>/02/01<span class="w"> </span><span class="m">22</span>:34:25<span class="w"> </span>routes.go:1187:<span class="w"> </span>INFO<span class="w"> </span>server<span class="w"> </span>config<span class="w"> </span><span class="nv">env</span><span class="o">=</span><span class="s2">&quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:8081 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\eng_m\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:25.996+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:432<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total blobs: 26&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:25.998+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:439<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total unused blobs removed: 0&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.000+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1238<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Listening on 127.0.0.1:8081 (version 0.5.7)&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.001+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1267<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Dynamic LLM libraries&quot;</span><span class="w"> </span><span class="nv">runners</span><span class="o">=</span><span class="s2">&quot;[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.001+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu.go:226<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;looking for compatible GPUs&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.002+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:167<span class="w"> </span><span class="nv">msg</span><span class="o">=</span>packages<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">1</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.002+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:214<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="nv">package</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">cores</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="nv">efficiency</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">threads</span><span class="o">=</span><span class="m">16</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-02-01T22:34:26.188+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-04f76f9a-be0a-544b-9a6f-8607b8d0a9ab<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.6<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.6<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA GeForce RTX 3060 Ti&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;8.0 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;7.0 GiB&quot;</span>
</code></pre></div></li>
</ul>
<hr />
<h3 id="2change-cache-directory">2.<strong>Change cache directory</strong></h3>
<h4 id="change-default-ollama-directory-symlink">Change default <code>.ollama</code> directory (Symlink)</h4>
<ul>
<li>
<p>To change the cache directory from Windows directory to a directory that exist in a network, you can symlink the <code>~/.
ollama</code> directory to a different location.</p>
</li>
<li>
<p>First move the <code>.ollama</code> directory to the new location:
<div class="highlight"><pre><span></span><code><span class="nb">Move-Item</span> <span class="s2">&quot;$env:USERPROFILE\.ollama&quot;</span> <span class="s2">&quot;&lt;your-new-directory&gt;&quot;</span> <span class="n">-Force</span>
</code></pre></div></p>
</li>
<li>
<p>Second, create a symlink to the new location:
<div class="highlight"><pre><span></span><code><span class="nb">New-Item</span> <span class="n">-ItemType</span> <span class="n">SymbolicLink</span> <span class="n">-Path</span> <span class="s2">&quot;$env:USERPROFILE\.ollama&quot;</span> <span class="n">-Target</span> <span class="s2">&quot;&lt;your-new-directory&gt;&quot;</span>
</code></pre></div>
PowerShell will complain if the old director in your user profile exists, so make sure you move it to the new
directory as shown above.</p>
</li>
<li>
<p>Restart Ollama to apply the changes.</p>
</li>
<li>to verify the changes, you can run the <code>serve</code> or the <code>list</code> command from ollama.
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>serve
</code></pre></div></li>
</ul>
<h2 id="8-common-issues-troubleshooting">8. Common Issues &amp; Troubleshooting</h2>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command not found</td>
<td>Restart the terminal, check<code>$PATH</code>.</td>
</tr>
<tr>
<td>GPU not used</td>
<td>Check<code>nvidia-smi</code>, install CUDA drivers.</td>
</tr>
<tr>
<td>API not responding</td>
<td>Ensure<code>ollama serve</code> is running.</td>
</tr>
</tbody>
</table>
<hr />
<p>This guide provides everything needed to run Ollama locally and integrate it with <code>llama-index</code>. Let me know if you need further customization! 🚀</p>
<h2 id="9errors">9.Errors</h2>
<h3 id="1address-not-permitted">1.address not permitted</h3>
<p>You might face the following error when you run the <code>ollama serve</code> command
<div class="highlight"><pre><span></span><code>Error:<span class="w"> </span>listen<span class="w"> </span>tcp<span class="w"> </span><span class="m">127</span>.0.0.1:11434:<span class="w"> </span>bind:<span class="w"> </span>Only<span class="w"> </span>one<span class="w"> </span>usage<span class="w"> </span>of<span class="w"> </span>each<span class="w"> </span>socket<span class="w"> </span>address<span class="w"> </span><span class="o">(</span>protocol/network<span class="w"> </span>address/port<span class="o">)</span><span class="w"> </span>is<span class="w"> </span>normally<span class="w"> </span>permitted.
</code></pre></div>
This error is due to the port <code>11434</code> is already in use, to solve this error, you can check which process is using this port by running the following command
<div class="highlight"><pre><span></span><code>netstat<span class="w"> </span>-ano<span class="w"> </span><span class="p">|</span><span class="w"> </span>findstr<span class="w"> </span>:11434
</code></pre></div>
for linux users, you can use the following command
<div class="highlight"><pre><span></span><code>netstat<span class="w"> </span>-ano<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>:11434
</code></pre></div></p>
<p>You will get the following output
<div class="highlight"><pre><span></span><code><span class="w">    </span>TCP<span class="w">    </span><span class="m">127</span>.0.0.1:11434<span class="w">        </span><span class="m">0</span>.0.0.0:0<span class="w">              </span>LISTENING<span class="w">       </span><span class="m">20796</span>
</code></pre></div>
Then you can kill the process by running the following command
<div class="highlight"><pre><span></span><code>taskkill<span class="w"> </span>/F<span class="w"> </span>/PID<span class="w"> </span><span class="m">20796</span>
</code></pre></div>
for linux users, you can use the following command
<div class="highlight"><pre><span></span><code><span class="nb">kill</span><span class="w"> </span>-9<span class="w"> </span><span class="m">20796</span>
</code></pre></div></p>
<p>Then
you will gee the following output
<div class="highlight"><pre><span></span><code>SUCCESS:<span class="w"> </span>The<span class="w"> </span>process<span class="w"> </span>with<span class="w"> </span>PID<span class="w"> </span><span class="m">20796</span><span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>terminated.
</code></pre></div></p>
<ul>
<li>Then you can run the <code>ollama serve</code> command again, you should see the following output
<div class="highlight"><pre><span></span><code><span class="m">2024</span>/11/22<span class="w"> </span><span class="m">23</span>:20:04<span class="w"> </span>routes.go:1189:<span class="w"> </span>INFO<span class="w"> </span>server<span class="w"> </span>config<span class="w"> </span><span class="nv">env</span><span class="o">=</span><span class="s2">&quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\eng_m\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.393+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:755<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total blobs: 28&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.395+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>images.go:762<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;total unused blobs removed: 0&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.397+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1240<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Listening on 127.0.0.1:11434 (version 0.4.1)&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.400+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>common.go:49<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Dynamic LLM libraries&quot;</span><span class="w"> </span><span class="nv">runners</span><span class="o">=</span><span class="s2">&quot;[cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.400+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu.go:221<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;looking for compatible GPUs&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.400+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:167<span class="w"> </span><span class="nv">msg</span><span class="o">=</span>packages<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">1</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.400+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu_windows.go:214<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="nv">package</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">cores</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="nv">efficiency</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">threads</span><span class="o">=</span><span class="m">16</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2024</span>-11-22T23:20:04.592+01:00<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:123<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-04f76f9a-be0a-544b-9a6f-8607b8d0a9ab<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.6<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.6<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA GeForce RTX 3060 Ti&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;8.0 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;7.0 GiB&quot;</span>
</code></pre></div></li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../api/utils.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Utils">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Utils
              </div>
            </div>
          </a>
        
        
          
          <a href="llama-cpp.html" class="md-footer__link md-footer__link--next" aria-label="Next: Using Llama.cpp for Local LLM Inference">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Using Llama.cpp for Local LLM Inference
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "search.highlight", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
    
  </body>
</html>